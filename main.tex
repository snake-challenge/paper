\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{fullpage}

\usepackage[backend=biber,doi=true,hyperref=auto]{biblatex}

\usepackage[dvipsnames]{xcolor}

\usepackage{enumitem}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{array}

\newcolumntype{R}[2]{%
  >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
  l%
  <{\egroup}%
}
\newcommand{\rot}[1]{%
  \ifdefined\HCode
    {#1}
  \else
    \multicolumn{1}{R{45}{1em}}{#1}%
  \fi
}

\usepackage{amsmath, amsthm}

\newtheorem*{experiment}{Experiment}
\newtheorem*{definition}{Definition}

\usepackage{authblk}
\renewcommand\Affilfont{\itshape\small}
\renewcommand\Authands{ and }

\usepackage[super]{nth}

\usepackage{xspace}

\PassOptionsToPackage{hyphens}{url}
\usepackage[colorlinks=true]{hyperref}

\newcommand{\myhref}[2]{\href{#1}{#2}\footnote{\url{#1}}}

\newcommand{\name}{\textsc{Snake}\xspace}

\title{The \name challenge: Sanitization algorithms under attack}

\author[1]{Tristan Allard}
\author[1,2]{Louis B\'eziaud}
\author[2]{S\'ebastien Gambs}

\affil[1]{Univ Rennes, CNRS, IRISA}
\affil[2]{Universit\'e du Qu\'eb\'ec \`a Montr\'eal}
\affil[ ]{\texttt{\{tristan.allard,louis.beziaud\}@irisa.fr, gambs.sebastien@uqam.ca}}
\affil[ ]{{\scriptsize Authors appear in alphabetical order}}

\date{\today}

\addbibresource{main.bib}

\begin{document}

\maketitle

\begin{abstract}
  Competitions and challenges have proven to be useful tools in the fields of security, cryptography or machine learning for stimulating research as well as generating and testing open-source implementations of state-of-the-art methods.
  While there were already some privacy challenges organized in the domain of data sanitization (also called data anonymization or privacy-preserving data publishing), they have mainly focused on the protection aspect.
  In this paper, we propose the \name{} challenges, a series of contests dedicated to the design of privacy attacks against data sanitization schemes.
  The conception of the \name{} challenges is such that they allow participants to concentrate their attacking efforts over a small set of well-chosen sanitization schemes.
  In addition, the proposed structure for the challenge is generic in the sense that the details of the contest can easily be instantiated at each edition (\emph{e.g.}, type of attack, dataset used and background knowledge of the adversary).
  In this first edition, we propose to focus on membership inference attacks over a set of well-known differentially-private synthetic data generation schemes.
  More precisely, the design of the first edition includes an instantiation of the generic structure, a dataset derived from the Census data, an evaluation of the success of each attack algorithm against varying targets and background knowledge, and the technical environment for supporting the contest.
  This paper describes our proposal and positions it with respect to previous privacy challenges.
\end{abstract}

\section{Introduction}

Competitions and challenges are commonly used both in the machine learning community -- for boosting the design and development of practical and efficient solutions to hard or new problems -- and in the security community -- for training purposes or for the evaluation of existing infrastructures.
In contrast, in the privacy community, there is not a long tradition of holding such challenges.
However, in recent years several competitions focusing on data sanitization algorithms (also called \emph{data anonymization algorithms} or \emph{privacy-preserving data publishing algorithms}) have been launched~\cite{boutet:hal-02512677,haspc2021,pwscup2021,ridgeway2021}.

Some of them, like the \emph{2018 Differential Privacy NIST Challenge}~\cite{ridgeway2021}, have focused primarily on the defense aspect.
More precisely, their main requirements were that the proposed algorithms have to (1) meet formal guarantees such as differential privacy, (2) achieve high utility levels on real-life cases, and (3) are efficient enough for being run on today's off-the-shelf computer systems.
Others, like the \emph{Hide-and-Seek challenge}~\cite{haspc2021}, the INSAnonym competition~\cite{boutet:hal-02512677} or the \myhref{https://www.iwsec.org/pws/2021/cup21_e.html}{PWSCUP}~\cite{pwscup2021} additionally consider attacks on the sanitized datasets generated by the participants.
More precisely, these competitions were generally composed of two phases, in which the first one is dedicated to the design of sanitization algorithms (usually focus on a particular type of data and use case) while the second one usually consists in attacking the data sanitized with the algorithms developed during the first phase.

While the sanitization phases of past challenges have been successful, in the sense that it has provided insights on the privacy/utility trade-offs, lead to implementations from state-of-the-art sanitization algorithms or even novel algorithms, the outcomes of the attack phases were usually more mitigated.
For example, the organizers of the Hide-and-Seek challenge have reported attack results equivalent to random guesses~\cite{haspc2021}.
We believe that one of the main reason for this is that in order to be successful, the attack phase must allow participants (1) to dedicate sufficient time to the design, implementation and testing of their attack strategies and (2) to focus on a few algorithms.

In this paper, we propose a series of privacy challenges called \name specifically tailored to attacks against sanitization algorithms.
More precisely, the general structure of the \name challenge provides the following salient features: it (1) concentrates the energy and expertise of participants against a small set of carefully chosen state-of-the-art sanitization algorithms, (2) gives sufficient time to participants to design and test carefully their attacks, (3) enables the exploration of a wide range of possible adversary models and background knowledge and (4) complements nicely the current sanitization competitions that focus on the sanitization part, thus resulting in a complete defense-attack pipeline.
The first edition of the challenge focuses on the \emph{Membership Inference Attack} setting~\cite{Hu2021MembershipIA,Stadler2020SyntheticD} over tabular data synthetically generated while satisfying differential privacy~\cite{Dwork2014TheAF}.
The adversarial background knowledge consists in the record(s) of the target(s) of the attack.
Further editions will showcase other inference attacks, adversarial
background knowledge, privacy models and sanitization algorithms.

The outline of this report is as follows.
First in Section~\ref{sec:general}, we describe the generic structure of the \name challenges.
Then in Section~\ref{sec:firsted}, we instantiate it by providing the details of the first edition.
Afterwards, we position the \name challenges in Section~\ref{sec:related} with respect to previous sanitization competitions before concluding in Section~\ref{sec:conclusion}.

\section{General structure of the \name challenges}
\label{sec:general}

\begin{table}
  \caption{The parameters of the framework that must be instantiated in order to design an edition of the \name challenges.}
  \label{tab:params}
  \begin{description}[align=right, leftmargin=!, labelwidth=4cm]
  \item[Number of tracks] The number of independent tracks of the edition being designed.

  \item[Track] A track is defined by a time period, a set of sanitization algorithms, a base dataset and a set of teams.

  \item[Team] A set of individuals collaborating together under the same name and registered to one or more tracks.

  \item[Time period] For each track, the time period over which the track runs.

  \item[Base dataset] For each track, the full dataset input by the sanitization algorithms (partially or totally).

  \item[Sanitization algorithms] For each track, the set of sanitization algorithms attacked.

  \item[Attack goal] The objective of the attack (\textit{e.g.}, membership inference, reconstruction attack or attribute inference).

  \item[Success measure] For each track, the measure used for computing the score of each team.
  \end{description}
\end{table}

Basically, an edition of the \name series of challenges consists in a set of independent \emph{tracks}.
Each of these tracks focuses on a set of \emph{sanitization algorithms under attack}.
Each of this track also relies on a \emph{base dataset} from which private datasets are extracted.
In addition, a set of \emph{competing teams} registers to a track and afterwards are ranked based on a \emph{success measure} on the subset of algorithms they attack.
The success measure typically reflects the strength of the attack.
The \emph{time period} for conducting a track must be long enough for letting each team explore, design and implement at least one attack.

The instantiation of the above framework (\emph{e.g.}, number of tracks, details of the success measure, algorithms under attack--see Table~\ref{tab:params}) might vary along tracks and editions, allowing to implement it over a diversity of settings.
For a concrete illustration, we detail afterwards the design of the first edition in Section~\ref{sec:firsted}.

\section{The \name challenge -- \texorpdfstring{\nth{1}}{First} edition}
\label{sec:firsted}

\subsection{General organization}

Table~\ref{tab:params-first} summarizes the parameters of \name\textsubscript{1} while Figure~\ref{fig:flow} gives an overview of the complete workflow.

\begin{table}
  \caption{Parameters of the first edition of the \name challenges.}
  \label{tab:params-first}
  \begin{description}[align=right, leftmargin=!, labelwidth=4cm]
  \item[Tracks]
    \begin{enumerate}[leftmargin=*]
    \item Main
    \end{enumerate}

  \item[Team] To be defined at runtime.

  \item[Base dataset] The CPS (Current Population Survey) data projected over a subset of dimensions (tabular data).
    Each execution of a sanitization algorithm takes as input a private dataset consisting of random samples from the base dataset.

  \item[Targets] Each target consists in the set of records of a random household containing at least 5 individuals\footnote{Summary statistics are available at \url{https://github.com/snake-challenge/snake-cps/}}.

  \item[Sanitization algorithms] \texttt{PrivBayes}~\cite{privbayes}, \texttt{MST}~\cite{mckenna_winning_2021}, \texttt{PATE-GAN}~\cite{Jordon2019PATEGANGS}.

  \item[Attack goal] Membership inference.

  \item[Success measure] For each team, number of combinations of competition parameters (sanitization algorithm, privacy parameter) on which the given team obtains the best membership advantage~\cite{yeom2018}.
  \end{description}
\end{table}

\begin{figure}
  \centering \includegraphics[width=.4\textwidth]{flow}
  \caption{Overview of the \name\textsubscript{1} workflow for a single parameterized algorithm}.
  \label{fig:flow}
\end{figure}

\paragraph{Structure.}
The first edition of the \name challenge, called $\name_{1}$ below, is organized jointly with \myhref{https://apvp23.sciencesconf.org/}{APVP'23}.
More precisely, the competition starts end of May and terminates end of August, with an early feedback and a discussion session during APVP'23 in mid-june.
Remote participation will also be possible.

\paragraph{Expected outcomes.}
To favor both the reproducibility and the accountability of the results of \name\textsubscript{1}, (1) the description of the attacks designed along \name\textsubscript{1} and their code will be published online under an open-source license (\emph{i.e.}, approved by the \myhref{https://opensource.org/}{Open Source Initiative}), and (2) complete information about \name\textsubscript{1} settings (\emph{i.e.}, teams and scores) will also be made public.
The choice of the exact open source license of each attack source code is left to the team having written the code.

\subsection{Description of the participants' tasks}

\paragraph{Attack algorithm.}
\name\textsubscript{1} focuses on \emph{membership inference attacks} on differentially-private synthetic data generation algorithms.
More precisely, each team has to design an attack algorithm as follows.
\begin{description}
\item [Input] We provide to the attack algorithms the following information:
  \begin{itemize}
  \item The synthetic dataset generated by an execution of the targeted sanitization algorithm over a private dataset.
  \item The base dataset from which the private dataset is sampled.
  \item The parameters of the execution of the sanitization algorithm attacked.
  \end{itemize}

\item [Output] The output of the attack algorithm is a single real $\in [0, 1]$ indicating the predicted probability of each target being within the private dataset or not.
\end{description}

We detail below the computation of the private datasets and of the targets, the sanitization algorithms under attack and the success metric of attacks.

\paragraph{Base dataset and private datasets.}

\name\textsubscript{1} makes use of the publicly available \emph{EPI CPS Basic Monthly} data provided by the Economic Policy Institute~\cite{epi_microdata}.
The CPS dataset is divided in years in which a yearly dataset contains more than $10^6$ records and 125 columns\footnote{The description is available on the \myhref{https://microdata.epi.org/variables/}{EPI Microdata Extracts website}}.
A record contains information about a single individual in a household.
However, several individuals from the same household can be pooled and in addition any household can join the CPS at any given month and is interviewed (1) during $4$ consecutive months for the given year, and (2) the same $4$ consecutive months during the following year.

Our base dataset is built as follows.
We concatenate the years 2005 to 2022\footnote{This selection is motivated by the availability and consistency of the features.}.
For each household, we keep only the month\footnote{We make the assumption that households' identifiers are linkable across months.} in which it possesses the most records (\emph{i.e.}, individuals). We end up with each household being unique by year and month.
Afterwards, we drop all rows that have at least one missing value.
This results in a dataset that includes 77111 households and 201279 rows (\emph{i.e.}, individuals). Finally, we project the dataset on 15 attributes (see Table~\ref{tab:attributes}), which results in what is called the base dataset.

\begin{table}
  \caption{Attributes' description of the base dataset}
  \label{tab:attributes}
 \begin{tabular}{*{16}{l}}
                     & 
     \rot{age}       & 
     \rot{agechild}  & 
     \rot{citistat}  & 
     \rot{female}    & 
     \rot{married}   & 
     \rot{ownchild}  & 
     \rot{wbhaom}    & 
     \rot{gradeatn}  & 
     \rot{cow1}      & 
     \rot{ftptstat}  & 
     \rot{statefips} & 
     \rot{hoursut}   & 
     \rot{faminc}    & 
     \rot{mind16}    & 
     \rot{mocc10}                                                                                                \\ \midrule
    Numerical        & X          &    &   &   &   & X         &   &    &   &   &    & X          &    &    &    \\
    Ordinal          & X          &    &   &   &   & X         &   & X  &   &   &    & X          & X  &    &    \\
    Unique           & 65         & 16 & 5 & 2 & 2 & 12        & 6 & 16 & 8 & 9 & 51 & 129        & 15 & 16 & 10 \\
    Range            & $[16, 80]$ &    &   &   &   & $[0, 11]$ &   &    &   &   &    & $[0, 198]$ &    &    &    \\
  \end{tabular}
\end{table}

From this base dataset, we generate one private dataset for each parameterized sanitization algorithm attacked.
More precisely, each private dataset is obtained by combining a random subset of targets (\emph{i.e.}, the members) with $10^4$ \emph{individuals} sampled uniformly at random from the base data.

\paragraph{Targets and background knowledge.}
In \name\textsubscript{1}, any household that contains at least 5 individuals might be a target, with the target consisting of the full set of records of the household.
The set of targets given to a team for launching its membership inference attack on a given sanitization algorithm are extracted from the corresponding private dataset.

\name\textsubscript{1} considers the following background knowledge about each target.
The adversary knows (1) the exact records of the household targeted, and (2) the full base dataset\footnote{It gives teams the knowledge of the population distribution commonly assumed in
  membership inference attacks.}.
Additionally, following Kerckhoffs's principle, the adversary is also given the information about the sanitization algorithm targeted as well as the parameters used for the executions and has access to its implementation.
However, the randomness generated internally during the execution of the algorithm is unknown to the adversary (\emph{e.g.}, for the generation of the Laplace noise). %

\paragraph{Sanitization algorithms under attacks.}
The sanitization algorithms under attack during \name$_1$ are differentially-private synthetic data generation algorithms.
More precisely, we have selected a set of algorithms according to the following two criteria: technical soundness assessed by a rigorous peer-selection process (\emph{e.g.}, published at top-tier conferences or winner of a dedicated competition) and available open-source implementation.
In particular, we have used the implementation available in the \myhref{https://github.com/alan-turing-institute/reprosyn}{reprosyn package}.
Except for parameters related to differential privacy, we use the default values set in their implementations.


\begin{description}
\item[The \texttt{PrivBayes} algorithm~\cite{privbayes}] generates
  synthetic data by capturing the underlying distribution of the
  private data through a specific Bayesian network while satisfying $\epsilon$-differential privacy.
  \texttt{PrivBayes} is divided in two main steps, with half of the privacy budget being used for each step.
  First, it greedily constructs a low-degree Bayesian network by selecting the
  child/parents pairs maximizing the mutual information based on the
  Exponential mechanism\cite{Dwork2014TheAF}.
  Second, it computes the conditional distributions of the resulting
  network, perturbed by the Laplace mechanism.
  The synthetic data is finally generated by sampling iteratively from the conditional
  distributions, which is a form of post-processing preserving the differential privacy guarantees.

\item[The \texttt{MST} algorithm~\cite{mckenna_winning_2021}] is a
  generalization of the NIST-MST algorithm, which has won the 2018
  \myhref{https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2018-differential-privacy-synthetic}{NIST Differential Privacy Synthetic Data challenge}.
  It generates synthetic data by perturbing the marginals that capture the data distribution through the Gaussian mechanism and by
  post-processing them through the \texttt{Private-PGM}
  algorithm~\cite{privatepgm19}.
  The base marginals can be obtained by one of the three following methods: given by a domain
  expert, computed from a public dataset that follows a data
  distribution similar to the private dataset's distribution or
  computed from the private dataset while satisfying differential
  privacy.
  Afterwards, \texttt{Private-PGM} takes as input the
  perturbed marginals and computes a data distribution that would have
  produced close marginals.
  More precisely, it represents the problem
  as an optimization problem, the distribution searched as a graphical
  model, and thus solves the problem of finding the optimal graphical
  model.
  Synthetic data can then be generated based on the model
  found.
  The \texttt{MST} algorithm has been demonstrated to satisfy
  $(\epsilon, \delta)$-differential privacy.

\item[The \texttt{PATE-GAN} algorithm~\cite{Jordon2019PATEGANGS}] is
  an extension of \emph{generative adversarial networks}~\cite{goodfellow2014generative} (GAN) based
  on the \emph{private aggregation of teacher ensembles} framework~\cite{Papernot2016SemisupervisedKT}
  (PATE).
  Each \texttt{PATE-GAN} iteration consists in three phases.
  First, \texttt{PATE-GAN} trains $k$ classifiers (called teachers)
  for distinguishing private real data from generated synthetic data.
  Each teacher is fed with samples from the real dataset (one
  partition per teacher) and from a generator (uniform distribution at
  first) and learns to distinguish between the two.
  Second, \texttt{PATE-GAN} builds on the $k$ teachers for training the
  student (a binary classifier as well) and the generator in an
  adversarial manner.
  The generator produces a set of samples but the teachers are now used for voting, for each sample, whether it is realistic or fake.
  The number of votes for each label (realistic or
  fake) is perturbed by the Laplace mechanism and the final label of a
  sample is the majority label.
  The student learns to distinguish between real and fake samples based on the resulting dataset.
  Third, the generator outputs a final set of samples and is penalized
  according to the success of the student in distinguishing realistic
  samples from fake samples.
  \texttt{PATE-GAN} iterates until the
  privacy budget is exhausted.
  This method has been shown to satisfy $(\epsilon, \delta)$-differential privacy.
\end{description}

\paragraph{Success measure.}
The success of a team is computed by first measuring the successes of its attack on each parameterized algorithm attacked (\textit{e.g.}, \texttt{MST} parameterized by ($\epsilon=1.0$, $\delta=10^{-5}$)) and second by aggregating the success measures in a single final score.
More precisely, the success of a given attack for a given triple is evaluated based on the well-known \emph{membership advantage measure}~\cite{yeom2018} (see Definition~\ref{def:membadv}).
Membership advantage is computed for each attack through the execution of a \emph{membership experiment} (see Experiment~\ref{exp:membexp}).
A single membership experiment requires to estimate the success probabilities of the attack and consequently to run the same attack $r$ times.
The parameter $r$ must be at the same time large enough for obtaining a sufficiently stable estimation and small enough for being practical (\emph{e.g.},
$r=100$).
\begin{experiment}[Membership experiment $\textsf{Exp}^{\textsf{M}}(\mathcal{A}, A, n, \mathcal{D})$, from~\cite{yeom2018}]
  \label{exp:membexp}
  Let $\mathcal{A}$ be an adversary, $A$ be a learning algorithm, $n$ be a positive integer, and $\mathcal{D}$ be a distribution over data points $(x,y)$. The \textup{membership experiment}
  proceeds as follows:
  \begin{enumerate}
  \item Sample $S \sim \mathcal{D}^n$, and let $A_S = A(S)$.
  \item Choose $b \leftarrow \{0,1\}$ uniformly at random.
  \item Draw $z \sim S$ if $b=0$, or $z \sim \mathcal{D}$ if $b=1$
  \item
    \textup{$\textsf{Exp}^{\textsf{M}}(\mathcal{A}, A, n,
      \mathcal{D})$} is 1 if $\mathcal{A}(z, A_S, n, \mathcal{D})=b$
    and 0 otherwise. $\mathcal{A}$ must output either 0 or 1.
  \end{enumerate}
\end{experiment}

\begin{definition}[Membership advantage, from~\cite{yeom2018}]
  \label{def:membadv}
  The \textup{membership advantage} of $\mathcal{A}$ is defined as

  \begin{equation*}
    \textup{\textsf{Adv}}^{\textup{\textsf{M}}}(\mathcal{A}, A, n, \mathcal{D})= 2~\textup{Pr}[\textup{\textsf{Exp}}^{\textup{\textsf{M}}}(\mathcal{A}, A, n, \mathcal{D})=1] - 1,
  \end{equation*}
  in which the probabilities are taken over the coin flips of
  $\mathcal{A}$, the random choices of $S$ and $b$, and the random data
  point $z \sim S$ or $z \sim \mathcal{D}$.

  Equivalently, the right-hand side can be expressed as the difference
  between $\mathcal{A}$'s true and false positive rates:
  \begin{equation*}
    \textup{\textsf{Adv}}^{\textup{\textsf{M}}}(\mathcal{A}, A, n, \mathcal{D}) = \textup{Pr}[\mathcal{A} = 0 | b = 0] - \textup{Pr}[\mathcal{A} = 0 | b = 1].
  \end{equation*}
\end{definition}

At the end of each track, each team obtains a set of success measures, one per parameterized algorithm attacked.
Teams cannot be ranked through a direct comparison of their success measures because two success measures are comparable only if they were obtained on the same parameterized algorithm.
As a result, we count for each team the number of times its success measure is higher than the success measures of all the other teams' on the same algorithm and the same privacy parameters, and we rank the teams accordingly.

\subsection{Technical environment}

\paragraph{Execution environment.}
The design, coding, and executions of attacks are performed locally by each team with its own resources.
As a result, there is no restriction on the computing environment used to develop the attacks and the choice of the programming language and the available resources are unconstrained.
Note however that we use and provide the Python environment used to prepare the tasks.
To compute the success measures, we request from each team a probability vector per membership experiment (\textit{i.e.}, its guesses).
The success measures of the teams are computed based on the probability vectors received.


\paragraph{Competitor's kit and submissions.}
Prior to the competition, a competitor's kit is made available for each team.
The kit contains all the necessary resources for each team (\emph{e.g.}, the data schema, the base dataset, the private datasets, the targets, scripts for computing the success measure and the final score).
We use the \myhref{https://www.codabench.org/}{CodaBench platform} for hosting the kits, receiving the teams' submissions, and ranking them. %
Public data~\cite{beziaud_louis_2023_7858326} is available in a \myhref{https://github.com/snake-challenge/snake-cps}{dedicated repository}.
All teams are required to have an account on CodaBench for participating to \name$_1$.

\paragraph{Communications between organizers and competitors.}
The official \myhref{https://snake-challenge.github.io/}{web site} hosts the call for participation, the registration form, the news and the final outcomes of the competition.
Besides this website, dedicated mailing-lists or other communication channels might be proposed to participants (\emph{e.g.},~forums or chats).


\paragraph{Reproducibility.} The random seed used to sample data (train and targets) is saved for future reproducibility.
The code used to build the whole challenge (tasks and CodaBench bundle) is available in the \myhref{https://github.com/snake-challenge/snake1}{snake1 repository}.

\paragraph{Verifiability.} Each task is distributed along with a hash of all its files (public and private). Once private data is shared (after the competition), this allows participants to verify that their submission was evaluated against the correct data.

\section{Overview of related challenges}
\label{sec:related}
The closest related challenges to ours are the \emph{Hide \& Seek privacy challenge}~\cite{haspc2021} from the Van Der Schaar laboratory, the \emph{INSAnonym competition}~\cite{boutet:hal-02512677}, the \emph{PWSCup}~\cite{pwscup2021} series of competitions, as well as the \myhref{https://github.com/microsoft/MICO}{Microsoft Membership Inference Competition (MICO)}.
The three former challenges differ on the rules, the data, and the tasks they propose, but they follow the same two-phase structure: a sanitization phase during which the participants implement a sanitization algorithm and execute it on the given dataset and an attack phase during which the sanitization algorithms are attacked.
The attack phase of these competitions is often short (either by design or in practice) and the
algorithms attacked are chosen by the participants.
The recent MICO Competition focuses on membership inference however it considers classification models.
The \name challenges aim at strengthening the attack phase.
First, the sanitization algorithms attacked are chosen from the literature and/or from the latest sanitization challenges (\emph{e.g.}, the NIST differential privacy challenges).
Second, the number of algorithms under attack proposed to participants can be chosen in order to remain small (notion of track). This helps participants to focus on their attack algorithm and contributes to a better coverage of the sanitization algorithms.
Third, the participants can be given a large amount of time to design and implement their attack algorithms.
Fourth, by focusing on the attacks and controlling precisely the algorithms being attacked, the \name challenge gives the opportunity to stress-test state-of-the-art sanitization algorithms systematically (\emph{e.g.}, privacy parameters).
In particular, we believe that the \name challenges complement nicely the other challenges dedicated to designing sanitization algorithms (\emph{e.g.}, the NIST differential privacy challenges).

\section{Conclusion}
\label{sec:conclusion}
We propose the \name challenges, a series of contests designed for stress-testing sanitization algorithms.
The general structure of the \name challenges is designed to concentrate the workforce of the participants on attacking a few selected algorithms from the state-of-the-art.
Thus, we believe that it complements nicely the current sanitization competitions that mostly focus on the defense part.
The first edition, held jointly with APVP'23, will focus on membership inference attacks over differentially-private synthetic data generation algorithms, a timely topic.
All outcomes (code, algorithms, success measures) will be made public following open-source principles.
Overall, we hope that the \name challenges can help gain understanding of the empirical privacy guarantees of sanitization algorithms and of their parameters and pave the way to a greater democratization of sanitization algorithms providing strong privacy guarantees.


\section*{Acknowledgments}
We would like to warmly thanks the students, engineers and researchers that have helped us to design the \name{} challenge: Nampoina Andriamilanto, Thomas Guyet, Pascale Jade-Vallot, Antoine Laurent, Matthieu Simonin and Margaux Tela.

\printbibliography[title={References}]

\end{document}
